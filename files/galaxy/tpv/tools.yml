---
# ALL tags must be with dashes (-) instead of underscores (_)
tools:
  basic_gpu_resource_param_tool:
    # Type of compute resource (CPU or GPU) for the tool depends on user's input from its wrapper.
    # Default resource is CPU.
    rules:
      - id: resource_params_gpu
        if: |
          param_dict = job.get_param_values(app)
          param_dict.get('__job_resource', {}).get('__job_resource__select') == 'yes'
        gpus: int(job.get_param_values(app)['__job_resource']['gpu'])

  __DATA_FETCH__:
    cores: 1
    mem: 3
    gpus: 0
    scheduling:
      require:
        - upload
    rules:
      - id: no-pulsar
        if: user is not None
        execute: |
            from tpv.core.entities import Tag, TagSetManager, TagType

            user_preferences = user.extra_preferences
            pulsar_tag = user_preferences.get("distributed_compute|remote_resources", "None")
            pulsar_tag = Tag("scheduling", pulsar_tag, TagType.REQUIRE) if pulsar_tag != "None" else None

            entity.tpv_tags.tags = [
                tag
                for tag in entity.tpv_tags.tags
                if tag != pulsar_tag
            ]

    env:
      TEMP: /data/1/galaxy_db/tmp

  toolshed.g2.bx.psu.edu/repos/chemteam/gmx_sim/gmx_sim/.*:
    inherits: basic_gpu_resource_param_tool

  toolshed.g2.bx.psu.edu/repos/bgruening/tabpfn/tabpfn/.*:
    inherits: basic_gpu_resource_param_tool
    # This tool is using git to get a huggingface model, it seems that git is easily getting into a locked state on NFS.
    # Here we reset the HOME dir in Galaxy to a non-NFS directory. It is true that now multiple tabpfn jobs are updating/cloning into a global directory,
    # but since its git, it should work and even be faster for jobs that already find a model. This is not vrey reproducible as the model can change, but this is
    # also true if the tool runs on NFS.
    env:
      HOME: /tmp
      TABPFN_ALLOW_CPU_LARGE_DATASET: 1

  toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_.*/deeptools_.*/.*:
    rules:
      - id: deeptools_singularity
        if: |
          # versions without singularity container available
          no_container = {
              '2.0.1.0',
              '2.1.0.0',
              '2.2.2.0',  # a container is available but it is broken
              '2.2.3.0',
              '2.5.0.0',
              '3.0.1.0',
              '3.0.2.0',
          }
          all(not helpers.tool_version_eq(tool, version) for version in no_container)
        scheduling:
          require:
            - singularity

  toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_compare/deeptools_bam_compare/.*:
    rules:
      - id: deeptools_singularity_bam_compare
        if: |
          # versions working in conda but not in singularity
          conda_only_versions = {
              '2.4.1.0',
          }
          any(helpers.tool_version_eq(tool, version) for version in conda_only_versions)
        # remove singularity tag
        execute: |
          from tpv.core.entities import TagSetManager, TagType
          entity.tpv_tags = TagSetManager([
              tag for tag in entity.tpv_tags.tags
              if not (tag.value == 'singularity' and tag.tag_type == TagType.REQUIRE)
          ])

  toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/.*:
    rules:
      - id: deeptools_singularity_bam_coverage
        if: |
          # versions working in conda but not in singularity
          conda_only_versions = {
              '2.4.1.0',
          }
          any(helpers.tool_version_eq(tool, version) for version in conda_only_versions)
        # remove singularity tag
        execute: |
          from tpv.core.entities import TagSetManager, TagType
          entity.tpv_tags = TagSetManager([
              tag for tag in entity.tpv_tags.tags
              if not (tag.value == 'singularity' and tag.tag_type == TagType.REQUIRE)
          ])

  toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_coverage/deeptools_plot_coverage/.*:
    rules:
      - id: deeptools_singularity_plot_coverage
        if: |
          # versions working in conda but not in singularity
          conda_only_versions = {
              '2.4.1.0',
          }
          any(helpers.tool_version_eq(tool, version) for version in conda_only_versions)
        # remove singularity tag
        execute: |
          from tpv.core.entities import TagSetManager, TagType
          entity.tpv_tags = TagSetManager([
              tag for tag in entity.tpv_tags.tags
              if not (tag.value == 'singularity' and tag.tag_type == TagType.REQUIRE)
          ])

  toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_profile/deeptools_plot_profile/.*:
    rules:
      - id: deeptools_singularity_plot_profile
        if: |
          # versions working in conda but not in singularity
          conda_only_versions = {
              '3.1.2.0.0',
          }
          any(helpers.tool_version_eq(tool, version) for version in conda_only_versions)
        # remove singularity tag
        execute: |
          from tpv.core.entities import TagSetManager, TagType
          entity.tpv_tags = TagSetManager([
              tag for tag in entity.tpv_tags.tags
              if not (tag.value == 'singularity' and tag.tag_type == TagType.REQUIRE)
          ])

  toolshed.g2.bx.psu.edu/repos/iuc/quast/quast/.*:
    cores: 10
    rules:
      - id: quast_memory
        # Quast has a vastly greater memory requirement when it generates alignments between scaffolds and a user-
        # provided reference. This is particularly true if the ref genome is large.
        if: |
          helpers.job_args_match(job, app, {"assembly": {"ref": {"use_ref": "true"}}, "large": True})
        mem: 250

  toolshed.g2.bx.psu.edu/repos/bgruening/hifiasm/hifiasm/.*:
    cores: 10
    rules:
      - id: hifiasm_memory
        # The memory requirement of Hifiasm depends on a wrapper's input
        if: |
          parameters = {p.name: p.value for p in job.parameters}
          parameters = tool.params_from_strings(parameters, app)

          advanced_options = parameters.get("advanced_options", dict())
          hg_size = advanced_options.get("hg_size", "")

          bool(hg_size)
        mem: |
          from math import ceil

          parameters = {p.name: p.value for p in job.parameters}
          parameters = tool.params_from_strings(parameters, app)

          advanced_options = parameters.get("advanced_options", dict())

          kcov_default = 36
          kcov = advanced_options.get("kcov", kcov_default)

          hg_size = advanced_options.get("hg_size", "")

          value = 0
          if hg_size:
              conversion_factors = {
                  "k": 1000000,
                  "M": 1000,
                  "G": 1,
              }
              conversion_factors = {
                  key.lower(): value for key, value in conversion_factors.items()
              }
              suffix = hg_size[-1:].lower()
              value = hg_size[:len(hg_size) - 1]
              value = value.replace(",", ".")
              value = float(value)
              # compute hg size in Gb
              value = value / conversion_factors[suffix.lower()]
              value = ceil(value * (kcov * 2) * 1.75)

          # return the amount of memory needed
          value

  toolshed.g2.bx.psu.edu/repos/bgruening/keras_train_and_eval/keras_train_and_eval/.*:
    inherits: basic_gpu_resource_param_tool

  toolshed.g2.bx.psu.edu/repos/iuc/snippy/snippy/.*:
    cores: 2
    scheduling:
      require:
        - condor-tpv
    rules:
      - if: input_size >= 0.015
        cores: 14
      - if: input_size >= 1
        cores: 14
        mem: 24

  toolshed.g2.bx.psu.edu/repos/iuc/enasearch_search_data/enasearch_search_data/.*:
    scheduling:
      require:
        - conda
        - singularity

  toolshed.g2.bx.psu.edu/repos/iuc/mummer_mummer/mummer_mummer/.*:
    mem: 8

  toolshed.g2.bx.psu.edu/repos/galaxy-australia/hifiasm_meta/hifiasm_meta/.*:
    cores: 8
    params:
      singularity_enabled: true

  toolshed.g2.bx.psu.edu/repos/rnateam/dewseq/dewseq/.*:
    cores: 2
    mem: 40
    scheduling:
      prefer:
        - condor-tpv

  toolshed.g2.bx.psu.edu/repos/iuc/fasta_stats/fasta-stats/.*:
    rules:
      - if: input_size >= 0.01
        cores: 3

  toolshed.g2.bx.psu.edu/repos/iuc/ncbi_fcs_gx/ncbi_fcs_gx/.*:
    rules:
      - id: ncbi_fcs_gx
        if: |
          helpers.job_args_match(job, app, {"mode": {"mode_selector": "screen"}})
        mem: 480
        cores: 64

  toolshed.g2.bx.psu.edu/repos/computational-metabolomics/sirius_csifingerid/sirius_csifingerid/.*:
    cores: 8
    mem: 8

  ## a user exported 100 of files and memory was not enough
  export_remote:
    mem: 8

  toolshed.g2.bx.psu.edu/repos/rnateam/htseq-clip/htseq-clip/.*:
    cores: 4
    mem: 16
    scheduling:
      prefer:
        - condor-tpv

  toolshed.g2.bx.psu.edu/repos/iuc/sleuth/sleuth/.*:
    cores: 4
    mem: 16
    scheduling:
      require:
        - conda
        - singularity

  toolshed.g2.bx.psu.edu/repos/iuc/rnaquast/rna_quast/.*:
    cores: 12
    mem: 40
    scheduling:
      require:
        - conda
        - singularity

  toolshed.g2.bx.psu.edu/repos/iuc/mashmap/mashmap/.*:
    cores: 8
    mem: 32
    scheduling:
      require:
        - conda
        - singularity

  toolshed.g2.bx.psu.edu/repos/galaxy-australia/alphafold2/alphafold/.*:
    cores: 10
    mem: 32
    rules:
      - if: helpers.tool_version_eq(tool, '2.0.0+galaxy1')
        # The version number of alphafold may not match the version number of the the tool in this case. The alphafold
        # version number should in fact be newer than 2.1.0 and older than 2.1.1. Check the links below to verify this
        # claim:
        # - https://github.com/usegalaxy-au/tools-au/blob/fae57866fda74c85d405a2db03a82ebfdaed6070/tools/alphafold/docker/Dockerfile#L7
        # - https://github.com/usegalaxy-au/tools-au/tree/fae57866fda74c85d405a2db03a82ebfdaed6070/tools/alphafold/docker
        # - https://github.com/usegalaxy-au/tools-au/commit/aa7c146a02df64fcaa8ef03a89a76012227f35d6
        # - https://github.com/deepmind/alphafold/blob/be37a41d6f83e4145bd4912cbe8bf6a24af80c29/setup.py#L21
        # However neither, 2.1.0 nor 2.1.1 are still supposed to be able to use the GPU during the relaxation step, yet
        # there are CUDA errors in the tool tests when no GPU is available. Verify the GPU use claim below:
        # - https://github.com/deepmind/alphafold/blob/be37a41d6f83e4145bd4912cbe8bf6a24af80c29/alphafold/relax/amber_minimize.py#L93
        # There seems indeed to have been a mishap when packaging this version of the tool. See the link below.
        # - https://github.com/usegalaxy-au/tools-au/commit/06dd35df4064c0c3c1272957e46c0df59d24c7fe
        # Regardless of how things came to be this way, this version of the tool needs a GPU, and the
        # requirement cannot be disabled) because `ALPHAFOLD_USE_GPU` is not declared in alphafold.xml. Thus we set
        # gpus to one.
        gpus: 1
      - if: helpers.tool_version_eq(tool, '2.0.0+galaxy2')
        # The same story as above applies to this tool version.
        # - https://github.com/usegalaxy-au/tools-au/blob/78302ce1d79058f37b24c7b395de450f42631260/tools/alphafold/alphafold.xml#L52
        gpus: 1
      - if: helpers.tool_version_eq(tool, '2.1.2+galaxy0')
        # This version of alphafold already allows to control whether the GPU should be used during the relaxation
        # step, but the tool developers added `ALPHAFOLD_USE_GPU` in the next tool revision (2.1.2+galaxy1). GPU use
        # is still mandatory.
        # - https://github.com/usegalaxy-au/tools-au/commit/6352c107873cf824d83bfe06b368523624746de7
        gpus: 1
      - if: helpers.tool_version_lt(tool, '2.3')
        params:
          singularity_run_extra_arguments: "--env ALPHAFOLD_DB=/data/db/databases/alphafold_databases/2.2/,ALPHAFOLD_USE_GPU=False"
      - if: helpers.tool_version_gte(tool, '2.3') and helpers.tool_version_lt(tool, '2.3.1+galaxy2')
        params:
          singularity_run_extra_arguments: "--env ALPHAFOLD_DB=/data/db/databases/alphafold_databases/2.3/,ALPHAFOLD_USE_GPU=False"
      - if: helpers.tool_version_gte(tool, '2.3.1+galaxy2')
        execute: |
          from random import SystemRandom
          if SystemRandom().random() < 0.01 and not job.get_param_values(app).get("model_preset") == "multimer": # Multimer needs more memory than we can currently provide on GPU nodes
            entity.gpus = 1
            entity.cores = 8
            # We are currently utilizing a whole GPU VM (cores 8 mem 40 gpus 1)
            entity.mem = 32
            entity.params['singularity_run_extra_arguments'] = '--env ALPHAFOLD_DB=/data/db/databases/alphafold_databases,ALPHAFOLD_USE_GPU=True'
          else:
            entity.params['singularity_run_extra_arguments'] = '--env ALPHAFOLD_DB=/data/db/databases/alphafold_databases,ALPHAFOLD_USE_GPU=False'
        # tweak amount of requested memory depending on the AlphaFold model to be run
      - id: model_preset_multimer
        if: job.get_param_values(app).get("model_preset") == "multimer"
        mem: 69
      - id: cz-pulsar_alphafold
        if: |
          "cz-pulsar" in {tag.value for tag in entity.tpv_tags.tags}
        gpus: 1
    scheduling:
      require:
        - singularity

  basic_docker_tool:
    params:
    scheduling:
      require:
        - docker

  toolshed.g2.bx.psu.edu/repos/bgruening/markitdown/markitdown/.*:
    inherits: basic_docker_tool
    params:
      docker_run_extra_arguments: --user 999

  toolshed.g2.bx.psu.edu/repos/goeckslab/scimap_anndata_to_csv/scimap_anndata_to_csv/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/goeckslab/scimap_mcmicro_to_anndata/scimap_mcmicro_to_anndata/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/goeckslab/scimap_phenotyping/scimap_phenotyping/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/goeckslab/scimap_plotting/scimap_plotting/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/goeckslab/scimap_spatial/scimap_spatial/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/bgruening/mitohifi/mitohifi/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/iuc/brew3r_r/brew3r_r/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/iuc/bellavista_prepare/bellavista_prepare/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/bgruening/vpt_extract/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/bgruening/vpt_segment/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/ecology/eml2eal/eml2eal/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/ecology/data_paper_from_eml/data_paper_from_EML/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/ecology/harmonize_insitu_to_netcdf/harmonize_insitu_to_netcdf/.*:
    inherits: basic_docker_tool
    env:
      _GALAXY_JOB_TMP_DIR: '/tmp'
  toolshed.g2.bx.psu.edu/repos/ecology/tool_odv/tool_odv/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/ecology/tool_odv_history/tool_odv_history/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/ecology/tool_biogeochemical_calibration/tool_biogeochemical_calibration/.*:
    inherits: basic_docker_tool
  # conda env not working, trying the container
  toolshed.g2.bx.psu.edu/repos/imgteam/color_deconvolution/ip_color_deconvolution/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/iuc/ggplot2_point/ggplot2_point/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/bgruening/mgnify_seqprep/mgnify_seqprep/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/artbio/manta/manta/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/bgruening/llm_hub/llm_hub/*:
    mem: 1
    env:
      LITELLM_CONFIG_FILE: '/opt/galaxy/config/litellm_config.yaml'
  toolshed.g2.bx.psu.edu/repos/goeckslab/squidpy/squidpy_spatial/*:
    inherits: basic_docker_tool
    mem: 6
    env:
      _GALAXY_JOB_TMP_DIR: '/tmp'
  toolshed.g2.bx.psu.edu/repos/goeckslab/gate_finder/gate_finder/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/galaxyp/qupath_roi_splitter/qupath_roi_splitter/.*:
    inherits: basic_docker_tool

  toolshed.g2.bx.psu.edu/repos/iuc/fileidentification/fileidentification/.*:
    inherits: basic_docker_tool

  toolshed.g2.bx.psu.edu/repos/bgruening/3dtrees_overviews/3dtrees_overviews/.*:
    inherits: basic_docker_tool
    cores: 5
    mem: 5

  toolshed.g2.bx.psu.edu/repos/bgruening/3dtrees_tile_merge/3dtrees_tile_merge/.*:
    inherits: basic_docker_tool
    cores: 10
    mem: 20

  toolshed.g2.bx.psu.edu/repos/bgruening/3dtrees_segmentanytree/3dtrees_segmentanytree/.*:
    inherits: basic_docker_tool
    gpus: 1
    cores: 10
    mem: 20

  toolshed.g2.bx.psu.edu/repos/bgruening/3dtrees_standardization/3dtrees_standardization/.*:
    inherits: basic_docker_tool
    cores: 10
    mem: 20

  toolshed.g2.bx.psu.edu/repos/bgruening/3dtrees_detailview/3dtrees_detailview/.*:
    inherits: basic_docker_tool
    gpus: 1
    cores: 8
    mem: 20

  3dtrees_sat:
    # /opt/galaxy/custom-tools/test/3dtrees/sat.xml
    inherits: basic_docker_tool
    gpus: 1
    cores: 8
    mem: 20

  3dtrees_overviews:
    # /opt/galaxy/custom-tools/test/3dtrees/overviews.xml
    inherits: basic_docker_tool

  3dtrees_standard:
    # /opt/galaxy/custom-tools/test/3dtrees/standard.xml
    inherits: basic_docker_tool

  3dtrees_tile_merge:
    # /opt/galaxy/custom-tools/test/3dtrees/tile_merge.xml
    inherits: basic_docker_tool
    cores: 10
    mem: 20

  3dtrees_detailview:
    # /opt/galaxy/custom-tools/test/3dtrees/detailview.xml
    inherits: basic_docker_tool
    gpus: 1
    cores: 8
    mem: 20

  toolshed.g2.bx.psu.edu/repos/galaxyp/msconvert/msconvert/.*:
    inherits: basic_docker_tool
    params:
      docker_run_extra_arguments: --user 999

  toolshed.g2.bx.psu.edu/repos/goeckslab/vitessce_spatial/vitessce_spatial/.*:
    inherits: basic_docker_tool
    params:
      docker_run_extra_arguments: --user 999
  toolshed.g2.bx.psu.edu/repos/bgruening/cellpose/cellpose/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/iuc/fgsea/fgsea/.*:
    # any container should work
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/iuc/cat_bins/cat_bins/.*:
    rules:
      - id: cat_bins_medium_input_rule
        if: input_size >= 0.03
        mem: 48
      - id: cat_bins_large_input_rule
        if: input_size >= 0.06
        mem: 72
  toolshed.g2.bx.psu.edu/repos/iuc/chewbbaca.*:
    inherits: basic_docker_tool

  # NCBI recommends AWS r6a.8xlarge: cores: 32, mem:256
  toolshed.g2.bx.psu.edu/repos/richard-burhans/ncbi_egapx/ncbi_egapx/.*:
    inherits: basic_docker_tool
    cores: 32
    mem: 248

  toolshed.g2.bx.psu.edu/repos/recetox/recetox_msfinder/recetox_msfinder/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/recetox/qcxms_getres/qcxms_getres/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/recetox/qcxms_neutral_run/qcxms_neutral_run/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/recetox/qcxms_production_run/qcxms_production_run/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/imgteam/bioformats2raw/bf2raw/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/bgruening/biomodels_biomd0000001066/biomodels_biomd0000001066/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/bgruening/biomodels_biomd0000001076/biomodels_biomd0000001076/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/bgruening/doclayoutyolo/doclayoutyolo/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/bgruening/json2yolosegment/json2yolosegment/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/bgruening/yolo_predict/yolo_predict/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/bgruening/yolo_training/yolo_training/.*:
    inherits: basic_docker_tool
    gpus: 1
    cores: 4
    mem: 32
    params:
      docker_run_extra_arguments: ' --gpus all --shm-size 16g '
    env:
      GPU_AVAILABLE: 1
      # the non-NFS /tmp dir is needed to avoid "resource is busy" errors
      _GALAXY_JOB_TMP_DIR: '/tmp'
  toolshed.g2.bx.psu.edu/repos/bgruening/parabricks_fq2bam/parabricks_fq2bam/.*:
    inherits: basic_docker_tool
    mem: 40
    gpus: 1
    cores: 16
    params:
      docker_env_CUDA_VISIBLE_DEVICES: "$_CONDOR_AssignedGPUs"
      docker_run_extra_arguments: ' --gpus all --env CUDA_VISIBLE_DEVICES=$_CONDOR_AssignedGPUs '
    context:
      exclude_gpu_models: ["Tesla V100-PCIE-32GB"]
      include_gpu_models: ["NVIDIA L40S", "Tesla T4"]
    env:
      GPU_AVAILABLE: 1
      # the non-NFS /tmp dir is needed to avoid "resource is busy" errors
      _GALAXY_JOB_TMP_DIR: '/tmp'
    scheduling:
      require:
        - embedded-pulsar
  toolshed.g2.bx.psu.edu/repos/imgteam/rfove/rfove/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/iuc/homer_findmotifs/homer_findMotifs/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/gga/repeatexplorer_clustering/repeatexplorer_clustering/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/galaxyp/diann/diann/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/bgruening/instagraal/instagraal/.*:
    inherits: basic_docker_tool
    gpus: 1
    cores: 1
    mem: 30
    params:
      docker_run_extra_arguments: ' --gpus all '
    env:
      GPU_AVAILABLE: 1

  # picrust tries to remove TMP folders, on NFS this does not always work, so we run this tool in a container with internal (non-NFS) TMP
  toolshed.g2.bx.psu.edu/repos/iuc/picrust.*:
    inherits: basic_docker_tool
    cores: 2
    mem: 10
    env:
      _GALAXY_JOB_TMP_DIR: '/tmp'

  toolshed.g2.bx.psu.edu/repos/iuc/diffdock/diffdock/.*:
    inherits: basic_docker_tool
    gpus: 1
    cores: 1
    params:
      docker_run_extra_arguments: ' --gpus all --user 999 '
    env:
      GPU_AVAILABLE: 1

  toolshed.g2.bx.psu.edu/repos/bgruening/black_forest_labs_flux/black_forest_labs_flux/.*:
    inherits: basic_docker_tool
    gpus: 1
    cores: 1
    params:
      ##docker_run_extra_arguments: ' --gpus all  --env CUDA_VISIBLE_DEVICES=$_CONDOR_AssignedGPUs '
      docker_run_extra_arguments: ' --gpus all '
    env:
      GPU_AVAILABLE: 1
    scheduling:
      require:
        - gpu-divided

  toolshed.g2.bx.psu.edu/repos/bgruening/whisper/whisper/.*:
    inherits: basic_docker_tool
    cores: 1
    mem: 5
    env:
      OPENAI_WHISPER_MODEL_DIR: "/data/db/models/openai_whisper/"

  toolshed.g2.bx.psu.edu/repos/bgruening/whisperx/whisperx/.*:
    inherits: basic_docker_tool
    gpus: 1
    cores: 1
    mem: 5
    params:
      docker_run_extra_arguments: '--gpus all --env WHISPERX_MODEL_DIR=$WHISPERX_MODEL_DIR --env WHISPERX_DEVICE=$WHISPERX_DEVICE --env WHISPERX_COMPUTE_TYPE=$WHISPERX_COMPUTE_TYPE'
    env:
      WHISPERX_MODEL_DIR: "/data/db/models/whisperx/whisperx_models/"
      WHISPERX_DEVICE: "cuda"
      WHISPERX_COMPUTE_TYPE: "float16"
    scheduling:
      require:
        - gpu-divided

  toolshed.g2.bx.psu.edu/repos/iuc/biapy/biapy/.*:
    inherits: basic_docker_tool
    gpus: 1
    cores: 1
    mem: 12
    params:
      docker_run_extra_arguments: ' --gpus all --shm-size 16g --env CUDA_VISIBLE_DEVICES=$_CONDOR_AssignedGPUs '
      docker_env_CUDA_VISIBLE_DEVICES: "$_CONDOR_AssignedGPUs"
      docker_env_GALAXY_BIAPY_GPU_STRING: "$_CONDOR_AssignedGPUs"
      docker_env_GPU_AVAILABLE: 1
    env:
      GPU_AVAILABLE: 1
      GALAXY_BIAPY_GPU_STRING: 0

  toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/.*:
    # 8 (from the shared TPV) seems to high for me
    cores: 3
    # https://github.com/usegalaxy-eu/issues/issues/837
    scheduling:
      require:
        - singularity
    params:
      # /tmp is not monted in, and is not writeable, this make fastqc crash
      singularity_run_extra_arguments: " -B /tmp:$TMPDIR "


  toolshed.g2.bx.psu.edu/repos/iuc/bbtools_tadpole/bbtools_tadpole/.*:
    mem: 8
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/iuc/pairtools_parse/pairtools_parse/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/ecology/cb_ivr/cb_ivr/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/ecology/taxo_cov_template/taxo_cov_template/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/ecology/makeeml/makeeml/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/ecology/entities_template/entities_template/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/ecology/eal_templates/eal_templates/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/ecology/eal_table_template/eal_table_template/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/ecology/raster_template/raster_template/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/ecology/vector_template/vector_template/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/perssond/basic_illumination/basic_illumination/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/goeckslab/mesmer/mesmer/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/perssond/quantification/quantification/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/perssond/s3segmenter/s3segmenter/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/perssond/coreograph/unet_coreograph/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/perssond/unmicst/unmicst/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/perssond/ashlar/ashlar/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/q2d2/qiime2_.*:
    inherits: basic_docker_tool
    params:
      docker_run_extra_arguments: --user 999:999

  toolshed.g2.bx.psu.edu/repos/bgruening/qiime2_dbotu_q2/.*:
    inherits: basic_docker_tool
  toolshed.g2.bx.psu.edu/repos/iuc/cherri_train/cherri_train/.*:
    inherits: basic_docker_tool
    cores: 10
    mem: 90

  toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_blastn_wrapper/.*:
    env:
      BLASTDB: "/data/db/databases/blast/taxdb"
  toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_blastp_wrapper/.*:
    env:
      BLASTDB: "/data/db/databases/blast/taxdb"
  toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_blastx_wrapper/.*:
    env:
      BLASTDB: "/data/db/databases/blast/taxdb"
  toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_makeblastdb/.*:
    env:
      BLASTDB: "/data/db/databases/blast/taxdb"
  toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_tblastn_wrapper/.*:
    env:
      BLASTDB: "/data/db/databases/blast/taxdb"
  toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_tblastx_wrapper/.*:
    env:
      BLASTDB: "/data/db/databases/blast/taxdb"

  toolshed.g2.bx.psu.edu/repos/iuc/velocyto_cli/velocyto_cli/.*:
    cores: 4
    mem: 100

  toolshed.g2.bx.psu.edu/repos/genouest/helixer/helixer/.*:
    cores: 4
    mem: 30
    scheduling:
      require:
        - singularity
    rules:
      - if: input_size >= 0.01
        gpus: 1
        params:
          singularity_run_extra_arguments: ' --nv '

  toolshed.g2.bx.psu.edu/repos/iuc/cherri_eval/cherri_eval/.*:
    inherits: basic_docker_tool
    cores: 1
    mem: 20

  toolshed.g2.bx.psu.edu/repos/nml/metaspades/metaspades/.*:
    cores: 2
    scheduling:
      accept:
        - pulsar
        - condor-tpv
    rules:
      - if: 0.05 <= input_size < 1
        cores: 8
        mem: 100
      - if: 1 <= input_size < 60
        cores: 16
        mem: 350
      - if: input_size >= 60
        fail: Too much data, please don't use Spades for this

  toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*:
    cores: 2
    rules:
      - if: 0.005 <= input_size < 1
        cores: 8
        mem: 100
      - if: 1 <= input_size < 2
        cores: 12
        mem: 200
      - if: 2 <= input_size < 20
        cores: 20
        mem: 350
        scheduling:
          prefer:
            - condor-tpv
      - if: input_size >= 20
        fail: Too much data, please don't use this tool for this.

  toolshed.g2.bx.psu.edu/repos/iuc/beagle/beagle/.*:
    cores: 1
    rules:
      - if: input_size < 1
        mem: 24
      - if: input_size >= 1
        mem: 64

  toolshed.g2.bx.psu.edu/repos/galaxy-australia/smudgeplot/smudgeplot/.*:
    cores: 8
    rules:
      - if: input_size < 1
        mem: 15
      - if: 1 <= input_size < 5
        mem: 75
      - if: 5 <= input_size < 10
        cores: 12
        mem: 150
      - if: 10 <= input_size < 15
        cores: 12
        mem: 225
      - if: 15 <= input_size < 20
        cores: 12
        mem: 300
      - if: 20 <= input_size < 25
        mem: 375
        cores: 16
      - if: input_size >= 25
        fail: Too much data, please check if the input is correct.

  toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*:
    # see https://github.com/usegalaxy-eu/infrastructure-playbook/pull/881 for some numbers
    cores: 10
    mem: 9 + input_size * 1

  toolshed.g2.bx.psu.edu/repos/iuc/shovill/shovill/.*:
    inherits: toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*

  toolshed.g2.bx.psu.edu/repos/iuc/spades_rnaviralspades/spades_rnaviralspades/.*:
    inherits: toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*

  toolshed.g2.bx.psu.edu/repos/iuc/rnaspades/rnaspades/.*:
    cores: 2
    rules:
      - if: 0.005 <= input_size < 1
        cores: 8
        mem: 100
      - if: 1 <= input_size < 2
        cores: 12
        mem: 200
      - if: input_size >= 2
        cores: 20
        mem: 350
        scheduling:
          prefer:
            - condor-tpv

  toolshed.g2.bx.psu.edu/repos/iuc/spades_plasmidspades/spades_plasmidspades/.*:
    inherits: toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*
  toolshed.g2.bx.psu.edu/repos/iuc/spades_metaviralspades/spades_metaviralspades/.*:
    inherits: toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*
  toolshed.g2.bx.psu.edu/repos/iuc/spades_metaplasmidspades/spades_metaplasmidspades/.*:
    inherits: toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*
  toolshed.g2.bx.psu.edu/repos/iuc/spades_coronaspades/spades_coronaspades/.*:
    cores: 10
    mem: 8
  toolshed.g2.bx.psu.edu/repos/iuc/spades_biosyntheticspades/spades_biosyntheticspades/.*:
    inherits: toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*

  # cactus suite
  toolshed.g2.bx.psu.edu/repos/galaxy-australia/cactus_cactus/cactus_cactus/.*:
    context:
      test_cores: 4
    cores: 20
    mem: 256
    scheduling:
      prefer:
        - condor-tpv
    params:
      singularity_enabled: true
  toolshed.g2.bx.psu.edu/repos/galaxy-australia/cactus_export/cactus_export/.*:
    params:
      singularity_enabled: true

  toolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/.*:
    cores: 24
    mem: 250
    scheduling:
      prefer:
        - condor-tpv
    rules:
      - if: input_size < 0.1
        cores: 1
        mem: 4
      - if: 0.1 <= input_size < 1
        cores: 12
        mem: 92
      - if: input_size >= 1
        fail: |
          Too much data, we cannot support such large Trinity assemblies. Please use RNAspades instead.

  '.*mothur_.*':
    cores: 1
    mem: 90
    params:
      docker_run_extra_arguments: --pids-limit 10000 --ulimit fsize=1000000000 --env TERM=vt100
      docker_volumes: "$_CONDOR_SCRATCH_DIR:rw,$job_directory:rw,$tool_directory:ro,$job_directory/outputs:rw,$working_directory:rw,/data/db/:ro,/data/dnb01/galaxy_db/:ro,/data/dnb02/galaxy_db/:ro,/data/dnb-ds03/galaxy_db/:ro,/data/dnb05/galaxy_db/:ro,/data/dnb06/galaxy_db/:rw,/data/dnb07/galaxy_db/:rw,/data/dnb08/galaxy_db/:rw,/data/dnb09/galaxy_db/:rw,/data/dnb10/galaxy_db/:rw,/data/dnb11/galaxy_db/:rw,/data/dnb12/galaxy_db/:rw,/data/dnb-ds02/galaxy_db/:ro,/data/dp01/galaxy_db/:rw,/data/0/galaxy_db/:ro,/data/1/galaxy_db/:ro,/data/2/galaxy_db/:ro,/data/3/galaxy_db/:ro,/data/4/galaxy_db/:ro,/data/5/galaxy_import/galaxy_user_data/:ro,/data/6/galaxy_db/:ro,/data/7/galaxy_db/:ro,/usr/local/tools/:ro"
      docker_default_container_id: centos:8.3.2011
    scheduling:
      require:
        - docker
        # see https://github.com/galaxyproject/galaxy/issues/16121#issuecomment-1555153421
        ##- embedded-pulsar

  '.*mothur_classify_seqs.*':
    cores: 2
    mem: 20
    params:
      docker_run_extra_arguments: --pids-limit 10000 --ulimit fsize=1000000000 --env TERM=vt100
      docker_volumes: "$_CONDOR_SCRATCH_DIR:rw,$job_directory:rw,$tool_directory:ro,$job_directory/outputs:rw,$working_directory:rw,/data/db/:ro,/data/dnb01/galaxy_db/:ro,/data/dnb02/galaxy_db/:ro,/data/dnb-ds03/galaxy_db/:ro,/data/dnb05/galaxy_db/:ro,/data/dnb06/galaxy_db/:rw,/data/dnb07/galaxy_db/:rw,/data/dnb08/galaxy_db/:rw,/data/dnb09/galaxy_db/:rw,/data/dnb10/galaxy_db/:rw,/data/dnb11/galaxy_db/:rw,/data/dnb12/galaxy_db/:rw,/data/dnb-ds02/galaxy_db/:ro,/data/dp01/galaxy_db/:rw,/data/0/galaxy_db/:ro,/data/1/galaxy_db/:ro,/data/2/galaxy_db/:ro,/data/3/galaxy_db/:ro,/data/4/galaxy_db/:ro,/data/5/galaxy_import/galaxy_user_data/:ro,/data/6/galaxy_db/:ro,/data/7/galaxy_db/:ro,/usr/local/tools/:ro"
      docker_default_container_id: centos:8.3.2011
    scheduling:
      require:
        - docker
        # see https://github.com/galaxyproject/galaxy/issues/16121#issuecomment-1555153421
        ##- embedded-pulsar

  '.*bioext_bam2msa.*':
    params:
      docker_run_extra_arguments: --pids-limit 10000 --ulimit fsize=1000000000 --env TERM=vt100
      docker_volumes: "$_CONDOR_SCRATCH_DIR:rw,$job_directory:rw,$tool_directory:ro,$job_directory/outputs:rw,$working_directory:rw,/data/db/:ro,/data/dnb01/galaxy_db/:ro,/data/dnb02/galaxy_db/:ro,/data/dnb-ds03/galaxy_db/:ro,/data/dnb05/galaxy_db/:ro,/data/dnb06/galaxy_db/:rw,/data/dnb07/galaxy_db/:rw,,/data/dnb08/galaxy_db/:rw,/data/dnb09/galaxy_db/:rw,/data/dnb10/galaxy_db/:rw,/data/dnb11/galaxy_db/:rw,/data/dnb12/galaxy_db/:rw,/data/dnb-ds02/galaxy_db/:ro,/data/dp01/galaxy_db/:rw,/data/0/galaxy_db/:ro,/data/1/galaxy_db/:ro,/data/2/galaxy_db/:ro,/data/3/galaxy_db/:ro,/data/4/galaxy_db/:ro,/data/5/galaxy_import/galaxy_user_data/:ro,/data/6/galaxy_db/:ro,/data/7/galaxy_db/:ro,/usr/local/tools/:ro"
      docker_default_container_id: centos:8.3.2011
    scheduling:
      require:
        - docker
        - embedded-pulsar

  'last_*':
    params:
      docker_run_extra_arguments: --pids-limit 10000 --ulimit fsize=1000000000 --env TERM=vt100
      docker_volumes: "$_CONDOR_SCRATCH_DIR:rw,$job_directory:rw,$tool_directory:ro,$job_directory/outputs:rw,$working_directory:rw,/data/db/:ro,/data/dnb01/galaxy_db/:ro,/data/dnb02/galaxy_db/:ro,/data/dnb-ds03/galaxy_db/:ro,/data/dnb05/galaxy_db/:ro,/data/dnb06/galaxy_db/:rw,/data/dnb07/galaxy_db/:rw,/data/dnb08/galaxy_db/:rw,/data/dnb09/galaxy_db/:rw,/data/dnb10/galaxy_db/:rw,/data/dnb11/galaxy_db/:rw,/data/dnb12/galaxy_db/:rw,/data/dp01/galaxy_db/:rw,/data/0/galaxy_db/:ro,/data/1/galaxy_db/:ro,/data/2/galaxy_db/:ro,/data/3/galaxy_db/:ro,/data/4/galaxy_db/:ro,/data/5/galaxy_import/galaxy_user_data/:ro,/data/6/galaxy_db/:ro,/data/7/galaxy_db/:ro,/usr/local/tools/:ro"
      docker_default_container_id: centos:8.3.2011
    scheduling:
      require:
        - docker
        - embedded-pulsar

  toolshed.g2.bx.psu.edu/repos/bgruening/blobtoolkit/blobtoolkit/.*:
    cores: 8
    mem: 20
    inherits: basic_docker_tool
    params:
      docker_run_extra_arguments: --user 999

  # 4GB is enough for most of the runs as it seems
  toolshed.g2.bx.psu.edu/repos/iuc/purge_dups/purge_dups/.*:
    cores: 1
    mem: 6

  toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MarkDuplicates/.*:
    cores: 8
    mem: 20
    inherits: basic_docker_tool
    params:
      docker_run_extra_arguments: --user 999

  toolshed.g2.bx.psu.edu/repos/bgruening/diamond/diamond/.*:
    cores: 6
    mem: 90
    rules:
      - if: input_size >= 30
        cores: 12
  toolshed.g2.bx.psu.edu/repos/bgruening/xchem_transfs_scoring/xchem_transfs_scoring/.*:
    scheduling:
      require:
        - docker
  toolshed.g2.bx.psu.edu/repos/bgruening/openduck_run_smd/openduck_run_smd/.*:
    env:
      docker_set_user: 1000
      docker_run_extra_arguments: '-e "OPENDUCK_GPU_PARAM=$OPENDUCK_GPU_PARAM" --gpus all'
    scheduling:
      require:
        - docker
  toolshed.g2.bx.psu.edu/repos/bgruening-util/stress_ng/stress_ng/.*:
    scheduling:
      require:
        - singularity
        - conda
  toolshed.g2.bx.psu.edu/repos/galaxyp/maxquant/maxquant/.*:
    scheduling:
      require:
        - singularity
  toolshed.g2.bx.psu.edu/repos/iuc/lumpy_prep/lumpy_prep/.*:
    scheduling:
      require:
        - singularity
        - conda
  # is there a way to avoid this
  ".*pcgr.*":
    mem: 16
    cores: 8
    env:
      GALAXY_PCGR_DIR: "/data/db/databases/pcgr"
    scheduling:
      require:
        - docker
  toolshed.g2.bx.psu.edu/repos/iuc/vardict_java/vardict_java/.*:
    scheduling:
      require:
        - singularity
        - conda
# Not for Pulsar, or is the file copied?
  toolshed.g2.bx.psu.edu/repos/climate/cds_essential_variability/cds_essential_variability/.*:
    env:
      COPERNICUS_CDSAPIRC_KEY_FILE: /data/db/data_managers/COPERNICUS_CDSAPIRC_KEY_FILE
  toolshed.g2.bx.psu.edu/repos/iuc/idr_download_by_ids/idr_download_by_ids/.*:
    scheduling:
      require:
        - singularity
        - conda
  toolshed.g2.bx.psu.edu/repos/imgteam/overlay_moving_and_fixed_image/ip_viz_overlay_moving_and_fixed_image/.*:
    cores: 8
  basic_numba_tool:
    env:
      NUMBA_CACHE_DIR: /data/2/galaxy_db/tmp
      OMP_NUM_THREADS: 4
      OPENBLAS_NUM_THREADS: 4
      MKL_NUM_THREADS: 4
      VECLIB_MAXIMUM_THREADS: 4
      NUMEXPR_NUM_THREADS: 4
      NUMBA_NUM_THREADS: 4
  toolshed.g2.bx.psu.edu/repos/computational-metabolomics/dimspy_process_scans/dimspy_process_scans/.*:
    inherits: basic_numba_tool
  toolshed.g2.bx.psu.edu/repos/computational-metabolomics/dimspy_replicate_filter/dimspy_replicate_filter/.*:
    inherits: basic_numba_tool
  toolshed.g2.bx.psu.edu/repos/computational-metabolomics/dimspy_align_samples/dimspy_align_samples/.*:
    inherits: basic_numba_tool
  toolshed.g2.bx.psu.edu/repos/bgruening/repeat_masker/repeatmasker_wrapper/.*:
    rules:
      - if: helpers.tool_version_gte(tool, '4.1.5')
        env:
          RM_LIB_PATH: "/data/db/databases/dfam/3.7/"
      - if: helpers.tool_version_lt(tool, '4.1.5')
        env:
          RM_LIB_PATH: "/data/db/databases/dfam/3.4/"
      - if: helpers.tool_version_gte(tool, '4.1.5')
        cores: 4

  toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/.*:
    inherits: basic_numba_tool

  toolshed.g2.bx.psu.edu/repos/iuc/scanpy_filter/scanpy_filter/.*:
    inherits: basic_numba_tool

  toolshed.g2.bx.psu.edu/repos/iuc/scanpy_inspect/scanpy_inspect/.*:
    inherits: basic_numba_tool

  toolshed.g2.bx.psu.edu/repos/iuc/scanpy_normalize/scanpy_normalize/.*:
    inherits: basic_numba_tool

  toolshed.g2.bx.psu.edu/repos/iuc/scanpy_remove_confounders/scanpy_remove_confounders/.*:
    inherits: basic_numba_tool

  toolshed.g2.bx.psu.edu/repos/iuc/scanpy_plot/scanpy_plot/.*:
    inherits: basic_numba_tool

  toolshed.g2.bx.psu.edu/repos/iuc/unicycler/unicycler/.*:
    cores: 24
    mem: 80
    env:
      TERM: vt100
    scheduling:
      accept:
        - pulsar
    rules:
      - id: unicycler_small_input_rule
        if: input_size < 0.05
        cores: 1
        mem: 3.8
      - id: unicycler_medium_input_rule
        if: 0.05 <= input_size < 2
        cores: 8
        mem: 28

  toolshed.g2.bx.psu.edu/repos/imgteam/unzip/unzip/.*:
    scheduling:
      require:
        - singularity

  # Also on add_to_tpv_shared_db.yml but without NUMBA_CACHE_DIR
  toolshed.g2.bx.psu.edu/repos/iuc/gemini_inheritance/gemini_inheritance/.*:
    inherits: basic_numba_tool

  toolshed.g2.bx.psu.edu/repos/iuc/chira_map/chira_map/.*:
    scheduling:
      require:
        - singularity
        - conda

  toolshed.g2.bx.psu.edu/repos/iuc/chira_merge/chira_merge/.*:
    scheduling:
      require:
        - singularity
        - conda

  toolshed.g2.bx.psu.edu/repos/iuc/chira_quantify/chira_quantify/.*:
    scheduling:
      require:
        - singularity
        - conda

  toolshed.g2.bx.psu.edu/repos/iuc/chira_extract/chira_extract/.*:
    scheduling:
      require:
        - singularity
        - conda

  toolshed.g2.bx.psu.edu/repos/galaxyp/custom_pro_db/custom_pro_db/.*:
    mem: 16

  toolshed.g2.bx.psu.edu/repos/iuc/data_manager_gtdbtk_database_installer/gtdbtk_database_installer/.*:
    mem: 128

  toolshed.g2.bx.psu.edu/repos/iuc/semibin/semibin/.*:
    mem: 32
    cores: 16

  toolshed.g2.bx.psu.edu/repos/iuc/comebin/comebin/.*:
    mem: 12
    cores: 32

  toolshed.g2.bx.psu.edu/repos/bgruening/nextdenovo/nextdenovo/.*:
    mem: 128
    cores: 24

  toolshed.g2.bx.psu.edu/repos/iuc/windowmasker/windowmasker_mkcounts/.*:
    mem: 64
    cores: 1

  jbrowse2:
    mem: 6
    cores: 1

  toolshed.g2.bx.psu.edu/repos/iuc/circos/circos/.*:
    scheduling:
      require:
        - singularity
    env:
      - name: SINGULARITYENV_LC_ALL
        value: C

  braker_tool:
    abstract: true
    rules:
      - id: braker_genome_sequence_count_rule
        if: |
          options = job.get_param_values(app)
          sequences = options["genome"].metadata.get("sequences")
          sequences > 100000
        fail: |
          The number of sequences in your genome file is too large (>100K) for this tool.

  toolshed.g2.bx.psu.edu/repos/genouest/braker3/braker3/3.0.6.*:
    inherits: braker_tool
    mem: 4
    cores: 6
    params:
      singularity_container_id_override: "docker://teambraker/braker3:v1.0.6"
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/genouest/braker3/braker3/3.0.7.*:
    inherits: braker_tool
    mem: 4
    cores: 6
    env:
      AUGUSTUS_CONFIG_PATH: "/opt/Augustus/config/"
    params:
      singularity_container_id_override: "docker://teambraker/braker3:v3.0.7.1"
      ## This should not be needed. Why are ENVs not passed into the container?
      singularity_cleanenv: true
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/genouest/braker3/braker3/3.0.8.*:
    inherits: braker_tool
    cores: 6
    mem: 4
    params:
      singularity_container_id_override: "docker://teambraker/braker3:v3.0.7.4"
      ## This should not be needed. Why are ENVs not passed into the container?
      singularity_cleanenv: true
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/genouest/braker3/braker3/3.0.3.*:
    inherits: braker_tool
    mem: 4
    cores: 6
    params:
      singularity_container_id_override: "docker://teambraker/braker3:v.1.0.4"
    scheduling:
      require:
        - singularity

  # GeneMark is a pain and needs special casing
  toolshed.g2.bx.psu.edu/repos/genouest/braker/braker/.*:
    inherits: braker_tool
    mem: 4
    cores: 6
    env:
      GENEMARK_PATH: "/usr/local/tools/genemark/etp.for_braker/bin/gmes/"
      PROTHINT_PATH: "/usr/local/tools/genemark/etp.for_braker/bin/gmes/ProtHint/bin/"

  toolshed.g2.bx.psu.edu/repos/iuc/checkm_taxonomy_wf/checkm_taxonomy_wf/.*:
    mem: 8
    cores: 8

  toolshed.g2.bx.psu.edu/repos/iuc/checkm_lineage_wf/checkm_lineage_wf/.*:
    mem: 16
    cores: 16
  toolshed.g2.bx.psu.edu/repos/iuc/maker/maker/.*:
    mem: 64
    cores: 16

  ## is in the shared DB - we need to test it here a bit
  toolshed.g2.bx.psu.edu/repos/galaxyp/openms_featurefindermultiplex/FeatureFinderMultiplex/.*:
    cores: 8
    mem: 64

  srma_wrapper:
    mem: 4
    cores: 1
    env:
      GALAXY_DATA_INDEX_DIR: "/opt/galaxy/tool-data/"
    params:
      singularity_container_id_override: "quay.io/biocontainers/mulled-v2-56685ecbcd8a92192501538ce5a05b1be490b742:1f13308a8260334d826a999ba1f77b563eccd8a5-0"
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/iuc/bakta/bakta/.*:
    cores: 8
    memory: 24
    scheduling:
      require:
        - conda
        - singularity

  toolshed.g2.bx.psu.edu/repos/iuc/compleasm/compleasm/.*:
    cores: 1
    mem: 40
    scheduling:
      require:
        - conda
        - singularity

  # 27:02:2024: We seem to have issues with the tools when loaded from the conda envs.
  # For whatever reason they seem to get stuck in the D state and never finish.
  # The below is an attempt to test whether by running the tools in a singularity container
  # we can avoid the issue.
  toolshed.g2.bx.psu.edu/repos/iuc/abricate/abricate/.*:
    mem: 30.4
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/galaxyp/eggnog_mapper/eggnog_mapper/.*:
    cores: 8
    mem: 24
    env:
      EGGNOG_DBMEM: "--dbmem"
    params:
      singularity_run_extra_arguments: "--env EGGNOG_DBMEM=--dbmem"
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/iuc/bwa_mem2/bwa_mem2/.*:
    cores: 32
    mem: 244
    rules:
      - id: bwa_mem2_small_input_rule
        if: input_size < 0.25
        cores: 2
        mem: 7.6
      - id: bwa_mem2_medium_input_rule
        if: 0.25 <= input_size < 16
        cores: 8
        mem: 28
      - id: bwa_mem2_large_input_rule
        if: 16 <= input_size < 32
        cores: 16
        mem: 58
      - id: bwa_mem2_xlarge_input_rule
        if: 32 <= input_size < 64
        cores: 24
        mem: 120
      - id: bwa_mem2_history_reference_rule
        if: |
          helpers.job_args_match(job, app, {"reference_source": {"reference_source_selector": "history"}})
        # per https://github.com/bwa-mem2/bwa-mem2/issues/41 it's 28 * reference
        mem: |
          options = job.get_param_values(app)
          size = options["reference_source"]["ref_file"].get_size()
          min(max(float(size/1024**3) * 28, (input_size - float(size/1024**3)) * 2, 7.6), 120)

  toolshed.g2.bx.psu.edu/repos/bgruening/flye/flye/.*:
    cores: 20
    mem: min(max(input_size * 1.2, 3.8), 256)
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/iuc/funannotate_annotate/funannotate_annotate/.*:
    inherits: basic_docker_tool
    cores: 8
    mem: 42
    env:
      _GALAXY_JOB_TMP_DIR: '/tmp'
    rules:
      - id: funannotate_annotate_large_input
        if: input_size > 3.2
        mem: 62

  toolshed.g2.bx.psu.edu/repos/iuc/funannotate_clean/funannotate_clean/.*:
    cores: 8
    mem: 40
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/iuc/funannotate_predict/funannotate_predict/.*:
    # we need this here because we need to set the GENEMARK_PATH
    cores: 12
    mem: 12
    env:
      GENEMARK_PATH: /usr/local/tools/genemark/etp.for_braker/bin/gmes/
      _GALAXY_JOB_TMP_DIR: '/tmp'
    params:
      singularity_run_extra_arguments: "--env GENEMARK_PATH=/usr/local/tools/genemark/etp.for_braker/bin/gmes/ -B /tmp:/tmp"
      singularity_no_mount: null
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/iuc/kraken2/kraken2/.*:
    cores: 16
    mem: 100
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_compute_graph/scanpy_compute_graph/.*:
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_filter_cells/scanpy_filter_cells/.*:
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_find_cluster/scanpy_find_cluster/.*:
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_find_variable_genes/scanpy_find_variable_genes/.*:
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_multiplet_scrublet/scanpy_multiplet_scrublet/.*:
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_normalise_data/scanpy_normalise_data/.*:
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_plot_embed/scanpy_plot_embed/.*:
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/ebi-gxa/scanpy_run_dpt/scanpy_run_dpt/.*:
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/bgruening/omark/omark/.*:
    cores: 8
    mem: 4
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/iuc/red/red/.*:
    cores: 1
    mem: 35

  toolshed.g2.bx.psu.edu/repos/iuc/anndata_manipulate/anndata_manipulate/.*:
    cores: 1
    mem: 16
    rules:
      - id: anndata_concat
        if: |
          param_dict = job.get_param_values(app)
          param_dict.get('manipulate', {}).get('function') == 'concatenate'
        mem: 60

  toolshed.g2.bx.psu.edu/repos/iuc/anndata_import/anndata_import/.*:
    cores: 1
    mem: 16

  toolshed.g2.bx.psu.edu/repos/iuc/anndata_export/anndata_export/.*:
    cores: 1
    mem: 16

  toolshed.g2.bx.psu.edu/repos/iuc/anndata_inspect/anndata_inspect/.*:
    cores: 1
    mem: 16

  toolshed.g2.bx.psu.edu/repos/iuc/modify_loom/modify_loom/.*:
    cores: 1
    mem: 16

  toolshed.g2.bx.psu.edu/repos/iuc/humann/humann/.*:
    cores: 12

  # Tool is same as fastqc so setting the same resources
  toolshed.g2.bx.psu.edu/repos/iuc/falco/falco/.*:
    cores: 3

  toolshed.g2.bx.psu.edu/repos/peterjc/tmhmm_and_signalp/Psortb/.*:
    scheduling:
      require:
        - conda
        - singularity
    env:
      _GALAXY_JOB_TMP_DIR: '/tmp'

  toolshed.g2.bx.psu.edu/repos/bgruening/hicexplorer_hicinfo/hicexplorer_hicinfo/.*:
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/galaxyp/fragpipe/fragpipe/.*:
    cores: 12
    mem: 256

  toolshed.g2.bx.psu.edu/repos/iuc/episcanpy_build_matrix/episcanpy_build_matrix/.*:
    env:
      _GALAXY_JOB_TMP_DIR: '/tmp'
    params:
      singularity_no_mount: null
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/iuc/optitype/optitype/.*:
    cores: 12
    mem: 256

  # ppanggolin all tries to remove TMP folders, on NFS this does not always work, so we run this tool in singularity with internal (non-NFS) TMP
  toolshed.g2.bx.psu.edu/repos/iuc/ppanggolin_all/ppanggolin_all/.*:
    env:
      _GALAXY_JOB_TMP_DIR: '/tmp'
    params:
      singularity_no_mount: null
    scheduling:
      require:
        - singularity

  toolshed.g2.bx.psu.edu/repos/iuc/vapor/vapor/.*:
    mem: 8

  toolshed.g2.bx.psu.edu/repos/iuc/seurat_preprocessing/seurat_preprocessing/.*:
    cores: 1
    mem: 20

  toolshed.g2.bx.psu.edu/repos/bgruening/lirasearch/lirasearch/.*:
    inherits: basic_docker_tool
    cores: 24
    mem: 24

  toolshed.g2.bx.psu.edu/repos/ecology/wildlife_megadetector_huggingface/wildlife_megadetector_huggingface/.*:
    inherits: basic_docker_tool
    gpus: 1
    mem: 20
